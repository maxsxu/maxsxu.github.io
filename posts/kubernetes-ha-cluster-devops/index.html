<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Max Xu">
    <meta name="description" content="Max Xu&#39;s personal website">
    <meta name="keywords" content="blog,software,engineer,developer,personal">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kubernetes 高可用集群和 DevOps 平台部署"/>
<meta name="twitter:description" content="Kubernetes 已然成为了云原生时代的操作系统，在容器管理、自动化编排等方面扮演着重要的角色。本文记录了部署 Kubernetes 高可用集群作为底层容器集群管理平台，并基于此"/>

    <meta property="og:title" content="Kubernetes 高可用集群和 DevOps 平台部署" />
<meta property="og:description" content="Kubernetes 已然成为了云原生时代的操作系统，在容器管理、自动化编排等方面扮演着重要的角色。本文记录了部署 Kubernetes 高可用集群作为底层容器集群管理平台，并基于此" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://atmax.io/posts/kubernetes-ha-cluster-devops/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-06-11T02:03:59+08:00" />
<meta property="article:modified_time" content="2020-11-11T11:50:45+08:00" />



    
      <base href="https://atmax.io/posts/kubernetes-ha-cluster-devops/">
    
    <title>
  Kubernetes 高可用集群和 DevOps 平台部署 · Max
</title>

    
      <link rel="canonical" href="https://atmax.io/posts/kubernetes-ha-cluster-devops/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://atmax.io/css/coder.min.74dded979ccea07b121f9effc955011ca567f89069980b995b7d2b04fea9fe00.css" integrity="sha256-dN3tl5zOoHsSH57/yVUBHKVn&#43;JBpmAuZW30rBP6p/gA=" crossorigin="anonymous" media="screen" />
    

    

    

    

    <link rel="icon" type="image/png" href="https://atmax.io/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://atmax.io/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.94.2" />

    
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-WWNFBZB');</script>
    

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-140227175-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-140227175-1');
    </script>

  </head>

  <body class=" ">
    
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWNFBZB"
      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    

    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://atmax.io">
      Max
    </a>
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://atmax.io/posts/">Blog</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://atmax.io/about/">About</a>
          </li>
        
      
      
    </ul>
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Kubernetes 高可用集群和 DevOps 平台部署</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2020-06-11T02:03:59&#43;08:00'>
                June 11, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              9 minutes read
            </span>
          </div>
          <div class="categories">
  <i class="fas fa-folder"></i>
    <a href="https://atmax.io/categories/cloudnative/">CloudNative</a></div>

          <div class="tags">
  <i class="fas fa-tag"></i>
    <a href="https://atmax.io/tags/kubernetes/">Kubernetes</a>
      <span class="separator">•</span>
    <a href="https://atmax.io/tags/devops/">DevOps</a></div>

        </div>
      </header>

      <div>
        <p>Kubernetes 已然成为了云原生时代的操作系统，在容器管理、自动化编排等方面扮演着重要的角色。本文记录了部署 Kubernetes 高可用集群作为底层容器集群管理平台，并基于此部署 DevOps 平台的整体流程。</p>
<h1 id="集群总览">集群总览</h1>
<p>集群一共有 7 个节点，各节点规划如下表:</p>
<table>
<thead>
<tr>
<th> </th>
<th>hostname</th>
<th>ip</th>
</tr>
</thead>
<tbody>
<tr>
<td>VIP</td>
<td> </td>
<td>10.219.12.8</td>
</tr>
<tr>
<td><strong>LoadBalancer</strong> (haproxy + keepalived)</td>
<td>k8s-10-129-12-18</td>
<td>10.219.12.18</td>
</tr>
<tr>
<td> </td>
<td>k8s-10-129-12-19</td>
<td>10.219.12.19</td>
</tr>
<tr>
<td><strong>Master</strong> (control plane)</td>
<td>k8s-10-129-12-13</td>
<td>10.219.12.13</td>
</tr>
<tr>
<td> </td>
<td>k8s-10-129-12-14</td>
<td>10.219.12.14</td>
</tr>
<tr>
<td> </td>
<td>k8s-10-129-12-15</td>
<td>10.219.12.15</td>
</tr>
<tr>
<td><strong>Minion</strong>/Worker/Node (data plane)</td>
<td>k8s-10-129-12-16</td>
<td>10.219.12.16</td>
</tr>
<tr>
<td> </td>
<td>k8s-10-129-12-17</td>
<td>10.219.12.17</td>
</tr>
</tbody>
</table>
<p>集群基于 stacked etcd 拓扑，总体架构如下图:</p>
<p><img src="https://atmax.io/images/kubernetes-ha-cluster-devops_2020-11-11-02-37-07.png" alt=""></p>
<p>每个控制平面节点都运行 <code>kube-apiserver</code>，<code>kube-scheduler</code> 和<code>kube-controller-manager</code> 的实例。使用负载均衡器将 <code>kube-apiserver</code> 暴露给工作节点。</p>
<p>每个控制平面节点都创建一个本地 <code>etcd</code> 成员，并且该 <code>etcd</code> 成员仅与此节点的 <code>kube-apiserver</code> 通信。这同样适用于本地 <code>kube-controller-manager</code> 和 <code>kube-scheduler</code>实例。</p>
<p>这种拓扑将相同节点上的控制平面和 <code>etcd</code> 成员耦合在一起。与具有外部 <code>etcd</code> 节点的集群相比，建立起来更容易，并且复制管理起来也更容易。</p>
<p>集群规划使用 3 个控制节点是达到 HA 的基本条件，为了防止“脑裂”的出现。</p>
<h1 id="准备">准备</h1>
<h2 id="系统规划">系统规划</h2>
<p>所有节点系统使用 UEFI + GPT 安装，使用 lvm2 分区，留一个裸磁盘或划分一个裸分区给 <code>ceph</code> 作为集群的底层存储。</p>
<h2 id="配置-ansible">配置 ansible</h2>
<p><code>Ansible</code> 用于集群节点的批量操作非常方便，自身是基于 Python 开发的。</p>
<p>使用 <code>k8s-10-129-12-18</code> 节点作为 ansible 管理节点。配置 <code>/etc/ansible/hosts</code> 文件:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-s" data-lang="s"><span style="display:flex;"><span>[k8s]
</span></span><span style="display:flex;"><span>k8s<span style="color:#ae81ff">-10-129-12</span><span style="color:#f92672">-</span>[13<span style="color:#f92672">:</span><span style="color:#ae81ff">17</span>]<span style="color:#f92672">:</span><span style="color:#ae81ff">2892</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[k8s<span style="color:#f92672">-</span>lb]
</span></span><span style="display:flex;"><span>k8s<span style="color:#ae81ff">-10-129-12</span><span style="color:#f92672">-</span>[18<span style="color:#f92672">:</span><span style="color:#ae81ff">19</span>]<span style="color:#f92672">:</span><span style="color:#ae81ff">2892</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[k8s<span style="color:#f92672">-</span>master]
</span></span><span style="display:flex;"><span>k8s<span style="color:#ae81ff">-10-129-12</span><span style="color:#f92672">-</span>[13<span style="color:#f92672">:</span><span style="color:#ae81ff">15</span>]<span style="color:#f92672">:</span><span style="color:#ae81ff">2892</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[k8s<span style="color:#f92672">-</span>worker]
</span></span><span style="display:flex;"><span>k8s<span style="color:#ae81ff">-10-129-12</span><span style="color:#f92672">-</span>[16<span style="color:#f92672">:</span><span style="color:#ae81ff">17</span>]<span style="color:#f92672">:</span><span style="color:#ae81ff">2892</span>
</span></span></code></pre></div><p>需要用到的 Ansible <code>ad-hoc</code> 命令:</p>
<ul>
<li>复制文件到所有节点: ansible -m copy -a &ldquo;src=test.txt dest=~/&rdquo;</li>
<li>所有节点安装软件: ansible -m apt -b -K -a &ldquo;name=chrony state=present&rdquo;</li>
<li>工作节点执行 shell 命令: ansible k8s-worker -m shell -a &ldquo;echo $PATH&rdquo;</li>
<li>sudo 权限执行: ansible k8s-worker -m shell -b -K -a &ldquo;ll /var/lib/docker&rdquo;</li>
<li>批量下载 docker 镜像
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>for i in `cat ~/gitlab.images`;do ansible k8s-worker -m shell -a &#34;docker pull $i&#34;;done
</span></span></code></pre></div></li>
</ul>
<h2 id="同步时钟">同步时钟</h2>
<p>执行 <code>ansible -m apt -b -K -a &quot;name=chrony state=present&quot;</code> 给所有节点安装 <code>chrony</code> 来同步时钟。底层时钟同步是分布式系统协同的保证。</p>
<h2 id="同步-hostname-和-hosts">同步 hostname 和 hosts</h2>
<ul>
<li><code>/etc/hostname</code></li>
</ul>
<p>k8s-10-219-12-13
k8s-10-219-12-14
k8s-10-219-12-15
k8s-10-219-12-16
k8s-10-219-12-17
k8s-10-219-12-18
k8s-10-219-12-19</p>
<ul>
<li><code>/etc/hosts</code></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>10.219.12.13  k8s-10-219-12-13.localhost k8s-10-219-12-13
</span></span></code></pre></div><h2 id="同步基础软件和软件源">同步基础软件和软件源</h2>
<ul>
<li>docker &amp;&amp; kubernetes</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># step 1: 安装必要的一些系统工具
</span></span><span style="display:flex;"><span>sudo apt update &amp;&amp; sudo apt install apt-transport-https ca-certificates curl software-properties-common -y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># step 2: 安装GPG证书
</span></span><span style="display:flex;"><span>curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
</span></span><span style="display:flex;"><span>curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># Step 3: 写入软件源信息
</span></span><span style="display:flex;"><span>sudo add-apt-repository &#34;deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&#34;
</span></span><span style="display:flex;"><span>sudo sh -c &#34;echo &#39;deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main&#39; &gt; /etc/apt/sources.list.d/kubernetes.list&#34;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># Step 4: 更新并安装
</span></span><span style="display:flex;"><span>sudo apt-mark unhold kubeadm kubectl kubelet
</span></span><span style="display:flex;"><span>sudo apt update &amp;&amp; sudo apt install docker-ce kubelet kubeadm kubectl -y
</span></span><span style="display:flex;"><span>sudo apt-mark hold kubeadm kubectl kubelet
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># Step 5: 清理
</span></span><span style="display:flex;"><span>sudo apt autoremove -y &amp;&amp; sudo apt autoclean -y
</span></span></code></pre></div><h2 id="更新-kubeadm-kubectl-kubelet">更新 kubeadm, kubectl, kubelet</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo apt-mark unhold kubeadm kubectl kubelet
</span></span><span style="display:flex;"><span>sudo apt update &amp;&amp; sudo apt upgrade -y
</span></span><span style="display:flex;"><span>sudo apt-mark hold kubeadm kubectl kubelet
</span></span></code></pre></div><h2 id="同步-docker-配置">同步 docker 配置</h2>
<p>为了提高 docker 的最大性能，编辑 <code>/etc/docker/daemon.json</code> 文件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span></span><span style="display:flex;"><span>  &#34;storage-driver&#34;: &#34;overlay2&#34;,
</span></span><span style="display:flex;"><span>  &#34;max-concurrent-downloads&#34;: 10,
</span></span><span style="display:flex;"><span>  &#34;max-concurrent-uploads&#34;: 10,
</span></span><span style="display:flex;"><span>  &#34;log-driver&#34;: &#34;json-file&#34;,
</span></span><span style="display:flex;"><span>  &#34;log-opts&#34;: {
</span></span><span style="display:flex;"><span>    &#34;max-size&#34;: &#34;10m&#34;,
</span></span><span style="display:flex;"><span>    &#34;max-file&#34;: &#34;1&#34;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>注意: 当 <code>max-file</code> 小于 2 或未设置 <code>max-size</code> 时<code>compress</code> 不能为 true.</p>
<h1 id="负载均衡节点配置">负载均衡节点配置</h1>
<p>使用 <code>haproxy</code> 和 <code>keepalived</code> 的组合实现负载均衡，作为 <code>kube-apiserver</code> 的代理，为整个集群提供流量入口。</p>
<p>直接使用集成了二者的 docker 镜像运行容器，启动负载均衡:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>HAPROXY_CONFIG_FILE=/root/haproxy/haproxy.cfg
</span></span><span style="display:flex;"><span>KEEPALIVED_CONFIG_FILE=/root/keepalived/keepalived.conf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>docker run -it -d --net=host --privileged \
</span></span><span style="display:flex;"><span>    -v ${HAPROXY_CONFIG_FILE}:/usr/local/etc/haproxy/haproxy.cfg \
</span></span><span style="display:flex;"><span>    -v ${KEEPALIVED_CONFIG_FILE}:/etc/keepalived/keepalived.conf \
</span></span><span style="display:flex;"><span>    --name kubernetes-proxy \
</span></span><span style="display:flex;"><span>    haproxy-keepalived
</span></span></code></pre></div><p><code>harproxy.cfg</code> 文件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>global
</span></span><span style="display:flex;"><span>    daemon
</span></span><span style="display:flex;"><span>    maxconn     4000
</span></span><span style="display:flex;"><span>    log         127.0.0.1 local2
</span></span><span style="display:flex;"><span>    pidfile     /var/run/haproxy.pid
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>defaults
</span></span><span style="display:flex;"><span>    mode                    http
</span></span><span style="display:flex;"><span>    log                     global
</span></span><span style="display:flex;"><span>    retries                 3
</span></span><span style="display:flex;"><span>    timeout connect         10s
</span></span><span style="display:flex;"><span>    timeout client          1m
</span></span><span style="display:flex;"><span>    timeout server          1m
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>listen kubernetes-master
</span></span><span style="display:flex;"><span>    bind *:6443
</span></span><span style="display:flex;"><span>    mode tcp
</span></span><span style="display:flex;"><span>    balance roundrobin
</span></span><span style="display:flex;"><span>    server master1 10.219.12.13:6443 check maxconn 2000
</span></span><span style="display:flex;"><span>    server master2 10.219.12.14:6443 check maxconn 2000
</span></span><span style="display:flex;"><span>    server master3 10.219.12.15:6443 check maxconn 2000
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>listen kubernetes-http
</span></span><span style="display:flex;"><span>    bind *:80
</span></span><span style="display:flex;"><span>    mode tcp
</span></span><span style="display:flex;"><span>    balance roundrobin
</span></span><span style="display:flex;"><span>    server master1 10.219.12.13:80 check maxconn 2000
</span></span><span style="display:flex;"><span>    server master2 10.219.12.14:80 check maxconn 2000
</span></span><span style="display:flex;"><span>    server master3 10.219.12.15:80 check maxconn 2000
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>listen kubernetes-https
</span></span><span style="display:flex;"><span>    bind *:443
</span></span><span style="display:flex;"><span>    mode tcp
</span></span><span style="display:flex;"><span>    balance roundrobin
</span></span><span style="display:flex;"><span>    server master1 10.219.12.13:443 check maxconn 2000
</span></span><span style="display:flex;"><span>    server master2 10.219.12.14:443 check maxconn 2000
</span></span><span style="display:flex;"><span>    server master3 10.219.12.15:443 check maxconn 2000
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>listen kubernetes-ssh
</span></span><span style="display:flex;"><span>    bind *:22
</span></span><span style="display:flex;"><span>    mode tcp
</span></span><span style="display:flex;"><span>    balance roundrobin
</span></span><span style="display:flex;"><span>    server master1 10.219.12.13:22 check maxconn 2000
</span></span><span style="display:flex;"><span>    server master2 10.219.12.14:22 check maxconn 2000
</span></span><span style="display:flex;"><span>    server master3 10.219.12.15:22 check maxconn 2000
</span></span></code></pre></div><p><code>keepalived.conf</code> 文件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>global_defs {
</span></span><span style="display:flex;"><span>   router_id LVS_DEVEL
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vrrp_script check_haproxy {
</span></span><span style="display:flex;"><span>    script &#34;pgrep haproxy&#34;
</span></span><span style="display:flex;"><span>    interval 2
</span></span><span style="display:flex;"><span>    weight -2
</span></span><span style="display:flex;"><span>    fall 10
</span></span><span style="display:flex;"><span>    rise 2
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vrrp_instance VI_1 {
</span></span><span style="display:flex;"><span>    state MASTER
</span></span><span style="display:flex;"><span>    interface ens160
</span></span><span style="display:flex;"><span>    virtual_router_id 51
</span></span><span style="display:flex;"><span>    priority 300
</span></span><span style="display:flex;"><span>    advert_int 1
</span></span><span style="display:flex;"><span>    authentication {
</span></span><span style="display:flex;"><span>        auth_type PASS
</span></span><span style="display:flex;"><span>        auth_pass 35f18af7190d51c9f7f78f37300a0cbd
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    virtual_ipaddress {
</span></span><span style="display:flex;"><span>        10.219.12.8
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    track_script {
</span></span><span style="display:flex;"><span>        check_haproxy
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h1 id="控制节点配置">控制节点配置</h1>
<p>创建 <code>kubeadm-config.yaml</code> 文件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>apiVersion: kubeadm.k8s.io/v1beta2
</span></span><span style="display:flex;"><span>kind: ClusterConfiguration
</span></span><span style="display:flex;"><span>kubernetesVersion: &#34;v1.17.4&#34;
</span></span><span style="display:flex;"><span>controlPlaneEndpoint: &#34;kubernetes.internal:6443&#34;
</span></span><span style="display:flex;"><span>imageRepository: &#34;registry.aliyuncs.com/google_containers&#34;
</span></span><span style="display:flex;"><span>networking:
</span></span><span style="display:flex;"><span>  podSubnet: 192.168.0.0/16
</span></span><span style="display:flex;"><span>apiServer:
</span></span><span style="display:flex;"><span>  extraArgs:
</span></span><span style="display:flex;"><span>    service-node-port-range: 22-32767
</span></span></code></pre></div><ul>
<li>无法访问 <code>k8s.gcr.io</code>，设置 <code>imageRepository</code> 属性</li>
<li>无法访问 <code>https://dl.k8s.io/release/stable-1.txt</code>，设置 <code>kubernetesVersion</code> 属性的具体版本，不要用 <code>stable</code></li>
</ul>
<h2 id="第一个控制节点初始化">第一个控制节点初始化</h2>
<p>选择 <code>k8s-10-219-12-13</code> 作为第一个控制节点，执行</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo kubeadm init --config=kubeadm-config.yaml --upload-certs
</span></span></code></pre></div><p>输出</p>
<p><img src="https://atmax.io/images/kubernetes-ha-cluster-devops_2020-11-11-02-59-43.png" alt=""></p>
<p>可以看到最后输出了一些帮助信息，分别是用于配置 <code>kubectl</code> 上下文的命令、用于添加工作节点的命令和用于添加其它控制节点的命令，记录下来稍后会用到。</p>
<h2 id="部署cni">部署CNI</h2>
<p>Kubernetes 的网络模型是一个大扁平的网络模型，需要部署 CNI 插件为集群提供网络服务。这里选择 <code>calico</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl apply -f calico.yaml
</span></span></code></pre></div><h2 id="其余控制节点加入">其余控制节点加入</h2>
<p>执行在初始化之后输出的添加命令，添加其它节点，执行后输出</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>To start administering your cluster from this node, you need to run the following as a regular user:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        mkdir -p $HOME/.kube
</span></span><span style="display:flex;"><span>        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
</span></span><span style="display:flex;"><span>        sudo chown $(id -u):$(id -g) $HOME/.kube/config
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Run &#39;kubectl get nodes&#39; to see this node join the cluster.
</span></span></code></pre></div><p>看到最后也给出了需要执行的命令，用于配置 <code>kubectl</code> 上下文。</p>
<p>如果 certificate-key 过期了，则在控制节点执行 <code>kubeadm init phase upload-certs --upload-certs</code> 即可。</p>
<h1 id="工作节点配置">工作节点配置</h1>
<p>所有控制节点加入到控制平面之后，就可以继续加入 worker 节点，在所有工作节点执行在初始化节点输出的加入工作节点命令，输出如下，注意没有使用到 certificate-key:</p>
<p><img src="https://atmax.io/images/kubernetes-ha-cluster-devops_2020-11-11-03-03-35.png" alt=""></p>
<p>执行 <code>kubectl get nodes</code> 可以看到现在集群已经配置完成，接下来开始部署应用。</p>
<h1 id="部署-helm">部署 Helm</h1>
<p><code>Helm</code> 被称为 Kubernetes 的包管理器，用于方便的部署其它云原生应用。</p>
<p>在节点 13 执行</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>wget https://get.helm.sh/helm-v3.1.2-linux-amd64.tar.gz &amp;&amp; sudo tar xzf helm-v3.1.2-linux-amd64.tar.gz -C /usr/local/bin/ --strip-components=1 linux-amd64/helm
</span></span></code></pre></div><p>添加阿里云镜像</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>helm repo add stable http://mirror.azure.cn/kubernetes/charts
</span></span><span style="display:flex;"><span>helm repo add apphub https://apphub.aliyuncs.com/
</span></span></code></pre></div><h1 id="部署-metallb">部署 MetalLB</h1>
<p>由于 Kubernetes 本身的网络方案中的 LB 功能仅限于提供商，所以需要第三方插件来实现 LB 网络的功能， <code>MetalLB</code> 就是这样一个插件。
部署到 <code>kube-network</code> 命名空间中</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl create namespace kube-network
</span></span></code></pre></div><p>下载 Chart 包</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>helm pull stable/metallb
</span></span></code></pre></div><p>编辑 <code>values.yaml</code> 文件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>configInline:
</span></span><span style="display:flex;"><span>    # Example ARP Configuration
</span></span><span style="display:flex;"><span>    address-pools:
</span></span><span style="display:flex;"><span>      - name: default
</span></span><span style="display:flex;"><span>        protocol: layer2
</span></span><span style="display:flex;"><span>        addresses:
</span></span><span style="display:flex;"><span>          - 10.219.12.200-10.219.12.250
</span></span></code></pre></div><p>安装</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>helm install metallb --namespace kube-network ./metallb
</span></span></code></pre></div><h1 id="部署-nginx-ingress">部署 nginx-ingress</h1>
<p><code>nginx-ingress</code> 用于提供 <code>Ingress</code> 服务。
部署到 <code>kube-network</code> 命名空间中</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl create namespace kube-network
</span></span></code></pre></div><p>安装</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>helm install nginx-ingress \
</span></span><span style="display:flex;"><span>  --namespace kube-network \
</span></span><span style="display:flex;"><span>  --set controller.image.repository=&#34;quay.azk8s.cn/kubernetes-ingress-controller/nginx-ingress-controller&#34; \
</span></span><span style="display:flex;"><span>  --set controller.service.nodePorts.http=80 \
</span></span><span style="display:flex;"><span>  --set controller.service.nodePorts.https=443 \
</span></span><span style="display:flex;"><span>  --set controller.service.nodePorts.tcp.22=22 \
</span></span><span style="display:flex;"><span>  --set defaultBackend.image.repository=&#34;gcr.azk8s.cn/google-containers/defaultbackend-amd64&#34; \
</span></span><span style="display:flex;"><span>  --set tcp.22=&#34;kube-gitlab/gitlab-gitlab-shell:22&#34; \
</span></span><span style="display:flex;"><span>  stable/nginx-ingress
</span></span></code></pre></div><p>设置默认 SSL 证书</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl -n kube-network create secret tls default-ssl-certificate --cert=internal-server.crt --key=internal-server.key
</span></span></code></pre></div><p>然后更新 nginx-ingress-controller Deployment 的 spec.containers.args 增加一个参数</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>- &#39;--default-ssl-certificate=kube-network/default-ssl-certificate&#39;
</span></span></code></pre></div><p>然后集群里的所有的 Ingress 都有证书了，甚至不用更改 Ingress 的任何配置。
新的 Ingress 不用指定 tls.secretName 即可</p>
<p><strong>注意</strong>
internal-server.crt 必须为证书链.
可以通过 Firefox 浏览器查看证书功能下载。</p>
<p>否则，Harbor registry 是无法登录的。因为 ingress 只返回了 server 证书，没有 intermediate 和 root 证书，所以无法信任。</p>
<h1 id="部署-metrics-server">部署 metrics-server</h1>
<p>部署 <code>HPA</code> (如部署 GitLab) 必须要有 metrics-server</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl create namespace kube-metrics
</span></span></code></pre></div><p>部署</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>helm upgrade --install metrics-server \
</span></span><span style="display:flex;"><span>  --namespace kube-metrics \
</span></span><span style="display:flex;"><span>  --set rbac.pspEnabled=true \
</span></span><span style="display:flex;"><span>  --set hostNetwork.enabled=false \
</span></span><span style="display:flex;"><span>  --set args[0]=&#34;--v=2&#34; \
</span></span><span style="display:flex;"><span>  --set args[1]=&#34;--kubelet-insecure-tls&#34; \
</span></span><span style="display:flex;"><span>  --set args[2]=&#34;--kubelet-preferred-address-types=InternalIP&#34; \
</span></span><span style="display:flex;"><span>  --set image.repository=&#34;gcr.azk8s.cn/google_containers/metrics-server-amd64&#34; \
</span></span><span style="display:flex;"><span>  stable/metrics-server
</span></span></code></pre></div><p>部署好之后，Dashboard 会新增 CPU 和 Memory 的指标图</p>
<h1 id="部署-kubernetes-dashboard">部署 Kubernetes Dashboard</h1>
<p><code>Dashboard</code> 是 K8s 官方提供的 GUI 工具，用于可视化查看集群的总体状态，还可以进行部署和扩缩容等操作。
部署到 <code>kube-public</code> 命名空间中</p>
<p>升级到使用 2.0</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>helm upgrade --install kubernetes-dashboard \
</span></span><span style="display:flex;"><span>  --namespace kube-public \
</span></span><span style="display:flex;"><span>  --set extraArgs[0]=&#34;--token-ttl=0&#34; \
</span></span><span style="display:flex;"><span>  --set ingress.enabled=true \
</span></span><span style="display:flex;"><span>  --set ingress.hosts[0]=&#34;k8s.internal&#34; \
</span></span><span style="display:flex;"><span>  --set ingress.tls[0].hosts[0]=&#34;k8s.internal&#34; \
</span></span><span style="display:flex;"><span>  --set metricsScraper.enabled=true \
</span></span><span style="display:flex;"><span>  kubernetes-dashboard/kubernetes-dashboard
</span></span></code></pre></div><p>由于设置了全局 ingress 证书，所以安装 dashboard 时无需指定证书的 secret.</p>
<p>2.0 版本默认去掉了 admin 权限，需要手动添加下设置 cluster-admin 权限
s</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>cat &lt;&lt; EOF | kubectl create -f -
</span></span><span style="display:flex;"><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style="display:flex;"><span>kind: ClusterRoleBinding
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: kubernetes-dashboard
</span></span><span style="display:flex;"><span>  namespace: kube-public
</span></span><span style="display:flex;"><span>roleRef:
</span></span><span style="display:flex;"><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style="display:flex;"><span>  kind: ClusterRole
</span></span><span style="display:flex;"><span>  name: cluster-admin
</span></span><span style="display:flex;"><span>subjects:
</span></span><span style="display:flex;"><span>  - kind: ServiceAccount
</span></span><span style="display:flex;"><span>    name: kubernetes-dashboard
</span></span><span style="display:flex;"><span>    namespace: kube-public
</span></span><span style="display:flex;"><span>EOF
</span></span></code></pre></div><h1 id="部署-rook">部署 Rook</h1>
<p><code>Rook</code> 是一个存储编排工具，可以方便的部署 <code>ceph</code> 集群。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl create namespace kube-storage
</span></span></code></pre></div><p>将部署到 15, 16, 17 三个节点，注意必须启用 rbd 内核模块</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo modprobe rbd
</span></span></code></pre></div><h2 id="部署-rook-ceph-operator">部署 rook-ceph Operator</h2>
<p>使用 <code>rook</code> 部署 <code>ceph</code> 集群是基于 <code>Operator</code> 实现的。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>helm repo add rook-release https://charts.rook.io/release
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>helm upgrade --install rook-ceph \
</span></span><span style="display:flex;"><span>  --namespace kube-storage \
</span></span><span style="display:flex;"><span>  --set image.repository=&#34;dockerhub.azk8s.cn/rook/ceph&#34; \
</span></span><span style="display:flex;"><span>  --set csi.cephcsi.image=&#34;quay.azk8s.cn/cephcsi/cephcsi:v2.0.0&#34; \
</span></span><span style="display:flex;"><span>  --set csi.registrar.image=&#34;quay.azk8s.cn/k8scsi/csi-node-driver-registrar:v1.2.0&#34; \
</span></span><span style="display:flex;"><span>  --set csi.provisioner.image=&#34;quay.azk8s.cn/k8scsi/csi-provisioner:v1.4.0&#34; \
</span></span><span style="display:flex;"><span>  --set csi.snapshotter.image=&#34;quay.azk8s.cn/k8scsi/csi-snapshotter:v1.2.2&#34; \
</span></span><span style="display:flex;"><span>  --set csi.attacher.image=&#34;quay.azk8s.cn/k8scsi/csi-attacher:v2.1.0&#34; \
</span></span><span style="display:flex;"><span>  --set csi.resizer.image=&#34;quay.azk8s.cn/k8scsi/csi-resizer:v0.4.0&#34; \
</span></span><span style="display:flex;"><span>  rook-release/rook-ceph
</span></span></code></pre></div><h2 id="创建-ceph-集群">创建 Ceph 集群</h2>
<p>OSD 的底层存储方式有两种 (storeType):</p>
<ul>
<li><code>filestore</code>。 用于目录.</li>
<li><code>bluestore</code>. 用于设备. 要单独挂一块硬盘 (如 /dev/sdc) 才能使用. 没有文件系统和分区的空硬盘设备。需要裸盘或裸分区</li>
</ul>
<p>设置 master 节点可调度</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>max@k8s-10-219-12-13:~$ kubectl taint node k8s-10-129-12-15 node-role.kubernetes.io/master-
</span></span><span style="display:flex;"><span>node/k8s-10-129-12-15 untainted
</span></span></code></pre></div><p><code>rook-cluster.yaml</code> 文件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>apiVersion: ceph.rook.io/v1
</span></span><span style="display:flex;"><span>kind: CephCluster
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: rook-ceph
</span></span><span style="display:flex;"><span>  namespace: kube-storage
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  ...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  storage: # cluster level storage configuration and selection
</span></span><span style="display:flex;"><span>    useAllNodes: true
</span></span><span style="display:flex;"><span>    useAllDevices: false
</span></span><span style="display:flex;"><span>    #deviceFilter:
</span></span><span style="display:flex;"><span>    config:
</span></span><span style="display:flex;"><span>      # The default and recommended storeType is dynamically set to bluestore for devices and filestore for directories.
</span></span><span style="display:flex;"><span>      # Set the storeType explicitly only if it is required not to use the default.
</span></span><span style="display:flex;"><span>      storeType: filestore
</span></span><span style="display:flex;"><span># Cluster level list of directories to use for filestore-based OSD storage. If uncomment, this example would create an OSD under the dataDirHostPath.
</span></span><span style="display:flex;"><span>    directories:
</span></span><span style="display:flex;"><span>      - path: /var/lib/storage
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    ...
</span></span></code></pre></div><p>创建</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl create -f rook-cluster.yaml
</span></span></code></pre></div><p>然后就可以发现在 15, 16, 17 三个节点创建了 <code>/var/lib/storage</code> 目录，结构如下</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>/var/lib/storage/
</span></span><span style="display:flex;"><span>├── kube-storage
</span></span><span style="display:flex;"><span>│   ├── client.admin.keyring
</span></span><span style="display:flex;"><span>│   ├── crash
</span></span><span style="display:flex;"><span>│   ├── kube-storage.config
</span></span><span style="display:flex;"><span>│   └── log
</span></span><span style="display:flex;"><span>├── mon-a
</span></span><span style="display:flex;"><span>│   └── data
</span></span><span style="display:flex;"><span>├── mon-b
</span></span><span style="display:flex;"><span>│   └── data
</span></span><span style="display:flex;"><span>└── mon-c
</span></span><span style="display:flex;"><span>    └── data
</span></span></code></pre></div><h2 id="创建-pool-和-storageclass">创建 Pool 和 StorageClass</h2>
<h3 id="cephblockpool">CephBlockPool</h3>
<p><code>rook-block-storageclass.yaml</code> 文件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-mysql" data-lang="mysql"><span style="display:flex;"><span>apiVersion: ceph.rook.io<span style="color:#f92672">/</span>v1
</span></span><span style="display:flex;"><span>kind: CephBlockPool
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: replicapool
</span></span><span style="display:flex;"><span>  namespace: kube<span style="color:#f92672">-</span>storage
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  failureDomain: host
</span></span><span style="display:flex;"><span>  replicated:
</span></span><span style="display:flex;"><span>    size: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span><span style="color:#75715e">--
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">apiVersion: storage.k8s.io/v1
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>kind: StorageClass
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>   name: rook<span style="color:#f92672">-</span>ceph<span style="color:#f92672">-</span>block
</span></span><span style="display:flex;"><span>   annotations:
</span></span><span style="display:flex;"><span>    storageclass.kubernetes.io<span style="color:#f92672">/</span><span style="color:#66d9ef">is</span><span style="color:#f92672">-</span><span style="color:#66d9ef">default</span><span style="color:#f92672">-</span>class: <span style="color:#e6db74">&#34;true&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Change &#34;rook-ceph&#34; provisioner prefix to match the operator namespace if needed
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>provisioner: kube<span style="color:#f92672">-</span>storage.rbd.csi.ceph.com
</span></span><span style="display:flex;"><span>parameters:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># clusterID is the namespace where the rook cluster is running
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    clusterID: kube<span style="color:#f92672">-</span>storage
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Ceph pool into which the RBD image shall be created
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    pool: replicapool
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># RBD image format. Defaults to &#34;2&#34;.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    imageFormat: <span style="color:#e6db74">&#34;2&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># RBD image features. Available for imageFormat: &#34;2&#34;. CSI RBD currently supports only `layering` feature.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    imageFeatures: layering
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The secrets contain Ceph admin credentials.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    csi.storage.k8s.io<span style="color:#f92672">/</span>provisioner<span style="color:#f92672">-</span>secret<span style="color:#f92672">-</span>name: rook<span style="color:#f92672">-</span>csi<span style="color:#f92672">-</span>rbd<span style="color:#f92672">-</span>provisioner
</span></span><span style="display:flex;"><span>    csi.storage.k8s.io<span style="color:#f92672">/</span>provisioner<span style="color:#f92672">-</span>secret<span style="color:#f92672">-</span>namespace: kube<span style="color:#f92672">-</span>storage
</span></span><span style="display:flex;"><span>    csi.storage.k8s.io<span style="color:#f92672">/</span>node<span style="color:#f92672">-</span>stage<span style="color:#f92672">-</span>secret<span style="color:#f92672">-</span>name: rook<span style="color:#f92672">-</span>csi<span style="color:#f92672">-</span>rbd<span style="color:#f92672">-</span>node
</span></span><span style="display:flex;"><span>    csi.storage.k8s.io<span style="color:#f92672">/</span>node<span style="color:#f92672">-</span>stage<span style="color:#f92672">-</span>secret<span style="color:#f92672">-</span>namespace: kube<span style="color:#f92672">-</span>storage
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Specify the filesystem type of the volume. If not specified, csi-provisioner
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e"># will set default as `ext4`.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">#csi.storage.k8s.io/fstype: xfs
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Delete the rbd volume when a PVC is deleted
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>reclaimPolicy: <span style="color:#66d9ef">Delete</span>
</span></span></code></pre></div><p>创建</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl -n kube-storage create -f rook-block-storageclass.yaml
</span></span></code></pre></div><p>标记默认 StorageClass</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl annotate storageclasses.storage.k8s.io rook-ceph-block storageclass.kubernetes.io/is-default-class=true
</span></span></code></pre></div><h3 id="cephfilesystem">CephFileSystem</h3>
<p><code>rook-filesystem-storageclass.yaml</code> 文件</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>apiVersion: ceph.rook.io/v1
</span></span><span style="display:flex;"><span>kind: CephFilesystem
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: cephfs
</span></span><span style="display:flex;"><span>  namespace: kube-storage
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  metadataPool:
</span></span><span style="display:flex;"><span>    replicated:
</span></span><span style="display:flex;"><span>      size: 3
</span></span><span style="display:flex;"><span>  dataPools:
</span></span><span style="display:flex;"><span>    - replicated:
</span></span><span style="display:flex;"><span>        size: 3
</span></span><span style="display:flex;"><span>  preservePoolsOnDelete: true
</span></span><span style="display:flex;"><span>  metadataServer:
</span></span><span style="display:flex;"><span>    activeCount: 1
</span></span><span style="display:flex;"><span>    activeStandby: true
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span>apiVersion: storage.k8s.io/v1
</span></span><span style="display:flex;"><span>kind: StorageClass
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: rook-cephfs
</span></span><span style="display:flex;"><span># Change &#34;rook-ceph&#34; provisioner prefix to match the operator namespace if needed
</span></span><span style="display:flex;"><span>provisioner: kube-storage.cephfs.csi.ceph.com
</span></span><span style="display:flex;"><span>parameters:
</span></span><span style="display:flex;"><span>  # clusterID is the namespace where operator is deployed.
</span></span><span style="display:flex;"><span>  clusterID: kube-storage
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  # CephFS filesystem name into which the volume shall be created
</span></span><span style="display:flex;"><span>  fsName: cephfs
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  # Ceph pool into which the volume shall be created
</span></span><span style="display:flex;"><span>  # Required for provisionVolume: &#34;true&#34;
</span></span><span style="display:flex;"><span>  pool: cephfs-data0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  # Root path of an existing CephFS volume
</span></span><span style="display:flex;"><span>  # Required for provisionVolume: &#34;false&#34;
</span></span><span style="display:flex;"><span>  # rootPath: /absolute/path
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
</span></span><span style="display:flex;"><span>  # in the same namespace as the cluster.
</span></span><span style="display:flex;"><span>  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
</span></span><span style="display:flex;"><span>  csi.storage.k8s.io/provisioner-secret-namespace: kube-storage
</span></span><span style="display:flex;"><span>  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
</span></span><span style="display:flex;"><span>  csi.storage.k8s.io/node-stage-secret-namespace: kube-storage
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>reclaimPolicy: Delete
</span></span></code></pre></div><p>创建</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl -n kube-storage create -f rook-filesystem-storageclass.yaml
</span></span></code></pre></div><p>s</p>
<h2 id="访问-ceph-dashboard">访问 Ceph Dashboard</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>max@k8s-10-219-12-13:~$ kubectl -n kube-storage get svc
</span></span><span style="display:flex;"><span>NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
</span></span><span style="display:flex;"><span>rook-ceph-mgr-dashboard    ClusterIP   10.101.182.69    &lt;none&gt;        8443/TCP            16h
</span></span></code></pre></div><p>默认是暴露一个 ClusterIP 类型的 Service。可以配置 Ingress  从集群外访问</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>apiVersion: networking.k8s.io/v1beta1
</span></span><span style="display:flex;"><span>kind: Ingress
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: rook-ceph-mgr-dashboard
</span></span><span style="display:flex;"><span>  namespace: kube-storage
</span></span><span style="display:flex;"><span>  annotations:
</span></span><span style="display:flex;"><span>    kubernetes.io/ingress.class: &#34;nginx&#34;
</span></span><span style="display:flex;"><span>    kubernetes.io/tls-acme: &#34;true&#34;
</span></span><span style="display:flex;"><span>    nginx.ingress.kubernetes.io/backend-protocol: &#34;HTTPS&#34;
</span></span><span style="display:flex;"><span>    nginx.ingress.kubernetes.io/server-snippet: |
</span></span><span style="display:flex;"><span>      proxy_ssl_verify off;
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  tls:
</span></span><span style="display:flex;"><span>   - hosts:
</span></span><span style="display:flex;"><span>     - ceph.internal
</span></span><span style="display:flex;"><span>     secretName: ceph.internal
</span></span><span style="display:flex;"><span>  rules:
</span></span><span style="display:flex;"><span>  - host: ceph.internal
</span></span><span style="display:flex;"><span>    http:
</span></span><span style="display:flex;"><span>      paths:
</span></span><span style="display:flex;"><span>      - path: /
</span></span><span style="display:flex;"><span>        backend:
</span></span><span style="display:flex;"><span>          serviceName: rook-ceph-mgr-dashboard
</span></span><span style="display:flex;"><span>          servicePort: https-dashboard
</span></span></code></pre></div><p>配置 ceph.internal 指向 10.219.12.8 就可以访问了</p>
<ul>
<li>默认用户名： <code>admin</code></li>
<li>默认密码: <code>kubectl -n kube-storage get secret rook-ceph-dashboard-password -o jsonpath=&quot;{['data']['password']}&quot; | base64 --decode &amp;&amp; echo</code></li>
</ul>
<h2 id="删除">删除</h2>
<blockquote>
<p><a href="https://rook.io/docs/rook/v1.2/ceph-teardown.html">https://rook.io/docs/rook/v1.2/ceph-teardown.html</a></p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># 1. Delete the Block and File artifacts
</span></span><span style="display:flex;"><span>kubectl delete -n kube-storage cephblockpool replicapool
</span></span><span style="display:flex;"><span>kubectl delete storageclass rook-ceph-block
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># 2. Delete the CephCluster CRD
</span></span><span style="display:flex;"><span>kubectl -n kube-storage delete cephcluster rook-ceph
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># 3. Delete the Operator and related Resources
</span></span><span style="display:flex;"><span>helm -n kube-storage uninstall rook-ceph
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># 4. Delete the daemonsets
</span></span><span style="display:flex;"><span>kubectl -n kube-storage delete daemonsets.apps csi-cephfsplugin
</span></span><span style="display:flex;"><span>kubectl -n kube-storage delete daemonsets.apps csi-rbdplugin
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># 4. Delete the data on hosts
</span></span><span style="display:flex;"><span>sudo rm -rf /var/lib/storage
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># 如果一直 Terminating, 强制删除 Pod
</span></span><span style="display:flex;"><span>kubectl -n kube-storage delete pod --all --force --grace-period=0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># 或直接强制删除 namespace
</span></span><span style="display:flex;"><span>kubectl delete namespace kube-storage --force --grace-period=0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># 删除 cluster-wide 资源
</span></span><span style="display:flex;"><span>kubectl delete psp 00-rook-ceph-operator
</span></span><span style="display:flex;"><span>kubectl delete crd objectbucketclaims.objectbucket.io
</span></span><span style="display:flex;"><span>kubectl delete crd objectbuckets.objectbucket.io
</span></span></code></pre></div><h1 id="部署-harbor">部署 Harbor</h1>
<p><code>Harbor</code> 提供了私有 registry 的功能，而且能够存储 <code>Helm Chart</code> 等云原生制品。这是由中国团队开发的，非常强大。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl create namespace kube-harbor
</span></span></code></pre></div><p>这里使用自有公钥创建 Secret <code>harbor-harbor-ingress</code>，不含 ca.crt 属性，这样 Harbor 不用下载证书。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>helm upgrade --install harbor \
</span></span><span style="display:flex;"><span>  --namespace kube-harbor \
</span></span><span style="display:flex;"><span>  --set expose.type=ingress \
</span></span><span style="display:flex;"><span>  --set expose.tls.enabled=true \
</span></span><span style="display:flex;"><span>  --set expose.tls.secretName=&#39;harbor-harbor-ingress&#39; \
</span></span><span style="display:flex;"><span>  --set expose.ingress.hosts.core=mirrors.internal \
</span></span><span style="display:flex;"><span>  --set expose.ingress.hosts.notary=notary.mirrors.internal \
</span></span><span style="display:flex;"><span>  --set externalURL=https://mirrors.internal \
</span></span><span style="display:flex;"><span>  --set notary.enabled=false \
</span></span><span style="display:flex;"><span>  --set clair.enabled=false \
</span></span><span style="display:flex;"><span>  --set trivy.enabled=false \
</span></span><span style="display:flex;"><span>  --set chartmuseum.enabled=true \
</span></span><span style="display:flex;"><span>  --set persistence.enabled=true \
</span></span><span style="display:flex;"><span>  harbor/harbor
</span></span></code></pre></div><ul>
<li>默认用户: <code>admin</code></li>
<li>默认密码: <code>Harbor12345</code></li>
</ul>
<p>Harbor 可选组件:</p>
<ul>
<li><code>Notary</code>: 用于镜像签名. 可以通过 <code>--set notary.enabled=false</code> 禁用</li>
<li><code>Clair</code>: 用于镜像漏洞扫描. 可以通过 <code>--set clair.enabled=false</code> 禁用</li>
</ul>
<h2 id="导出根证书">导出根证书</h2>
<p>有两种方式导出 CA 证书</p>
<ul>
<li>通过 GUI (Harbor portal 的系统管理 - 配置管理 - 系统设置 - 镜像库根证书下载) 下载</li>
<li>通过 kubectl 导出为 <code>mirrors.internal.ca.crt</code></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl -n kube-harbor get secrets harbor-harbor-ingress -o jsonpath=&#34;{.data.ca\.crt}&#34; | base64 --decode &gt; mirrors.internal.ca.crt
</span></span></code></pre></div><h2 id="分发证书">分发证书</h2>
<p><strong>Docker 证书设置</strong></p>
<p>所有节点执行</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo mkdir -p /etc/docker/certs.d/mirrors.internal
</span></span><span style="display:flex;"><span>sudo cp mirrors.internal.ca.crt /etc/docker/certs.d/mirrors.internal/ca.crt
</span></span></code></pre></div><p><strong>Helm 证书设置</strong></p>
<p>需要使用 <code>helm</code> 的客户端需要设置</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo cp mirrors.internal.ca.crt /etc/ssl/certs/mirrors.internal.ca.crt
</span></span></code></pre></div><h1 id="部署-gitlab">部署 GitLab</h1>
<p><code>DevOps</code> 平台功能由 <code>GitLab</code> 提供。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl create namespace kube-gitlab
</span></span></code></pre></div><p>使用 <code>helm</code> 部署</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>helm upgrade --install gitlab \
</span></span><span style="display:flex;"><span>  --namespace kube-gitlab \
</span></span><span style="display:flex;"><span>  --timeout 600s \
</span></span><span style="display:flex;"><span>  --set global.edition=ce \
</span></span><span style="display:flex;"><span>  --set global.hosts.domain=code.internal \
</span></span><span style="display:flex;"><span>  --set global.hosts.gitlab.name=code.internal \
</span></span><span style="display:flex;"><span>  --set global.hosts.gitlab.https=true \
</span></span><span style="display:flex;"><span>  --set global.hosts.registry.name=registry.code.internal \
</span></span><span style="display:flex;"><span>  --set global.hosts.registry.https=true \
</span></span><span style="display:flex;"><span>  --set global.hosts.minio.name=minio.code.internal \
</span></span><span style="display:flex;"><span>  --set global.hosts.minio.https=true \
</span></span><span style="display:flex;"><span>  --set global.ingress.class=nginx \
</span></span><span style="display:flex;"><span>  --set global.ingress.configureCertmanager=false \
</span></span><span style="display:flex;"><span>  --set nginx-ingress.enabled=false \
</span></span><span style="display:flex;"><span>  --set certmanager.install=false \
</span></span><span style="display:flex;"><span>  --set prometheus.install=true \
</span></span><span style="display:flex;"><span>  --set prometheus.alertmanager.enabled=false \
</span></span><span style="display:flex;"><span>  --set prometheus.kubeStateMetrics.enabled=false \
</span></span><span style="display:flex;"><span>  --set prometheus.nodeExporter.enabled=false \
</span></span><span style="display:flex;"><span>  --set prometheus.pushgateway.enabled=false \
</span></span><span style="display:flex;"><span>  --set gitlab-runner.install=false \
</span></span><span style="display:flex;"><span>  --set upgradeCheck.enabled=false \
</span></span><span style="display:flex;"><span>  gitlab/gitlab
</span></span></code></pre></div><p>输出如下:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-mysql" data-lang="mysql"><span style="display:flex;"><span>WARNING: Automatic TLS certificate generation <span style="color:#66d9ef">with</span> cert<span style="color:#f92672">-</span>manager <span style="color:#66d9ef">is</span> disabled <span style="color:#66d9ef">and</span> no TLS certificates were provided. Self<span style="color:#f92672">-</span>signed certificates were generated.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>You may retrieve the CA root <span style="color:#66d9ef">for</span> these certificates <span style="color:#66d9ef">from</span> the <span style="color:#f92672">`</span>gitlab<span style="color:#f92672">-</span>wildcard<span style="color:#f92672">-</span>tls<span style="color:#f92672">-</span>ca<span style="color:#f92672">`</span> secret, via the following command. It can <span style="color:#66d9ef">then</span> be imported <span style="color:#66d9ef">to</span> a web browser <span style="color:#66d9ef">or</span> system store.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    kubectl get secret gitlab<span style="color:#f92672">-</span>wildcard<span style="color:#f92672">-</span>tls<span style="color:#f92672">-</span>ca <span style="color:#f92672">-</span>ojsonpath<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;{.data.cfssl_ca}&#39;</span> <span style="color:#f92672">|</span> base64 <span style="color:#f92672">--</span>decode <span style="color:#f92672">&gt;</span> code.internal.ca.pem
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">If</span> you do <span style="color:#66d9ef">not</span> wish <span style="color:#66d9ef">to</span> <span style="color:#66d9ef">use</span> self<span style="color:#f92672">-</span>signed certificates, please <span style="color:#66d9ef">set</span> the following properties:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">-</span> global.ingress.tls.secretName
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">OR</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">-</span> global.ingress.tls.<span style="color:#a6e22e">enabled</span> (<span style="color:#66d9ef">set</span> <span style="color:#66d9ef">to</span> <span style="color:#f92672">`</span><span style="color:#66d9ef">true</span><span style="color:#f92672">`</span>)
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">-</span> gitlab.unicorn.ingress.tls.secretName
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">-</span> registry.ingress.tls.secretName
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">-</span> minio.ingress.tls.secretName
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>WARNING: <span style="color:#66d9ef">If</span> you are upgrading <span style="color:#66d9ef">from</span> a previous version of the GitLab Helm Chart, there <span style="color:#66d9ef">is</span> a major upgrade <span style="color:#66d9ef">to</span> the included PostgreSQL chart, which requires manual steps be performed. Please see our upgrade documentation <span style="color:#66d9ef">for</span> more information: https:<span style="color:#f92672">//</span>docs.gitlab.com<span style="color:#f92672">/</span>charts<span style="color:#f92672">/</span>installation<span style="color:#f92672">/</span>upgrade.html
</span></span></code></pre></div><ul>
<li>默认用户名: <code>root</code></li>
<li>默认密码: 在 Secret <code>gitlab-gitlab-initial-root-password</code></li>
</ul>
<h2 id="导出根证书-1">导出根证书</h2>
<ul>
<li>使用 kubectl 导出为 <code>code.internal.ca.crt</code></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>kubectl -n kube-gitlab get secrets gitlab-wildcard-tls-ca -ojsonpath=&#34;{.data.cfssl_ca}&#34; | base64 --decode &gt; code.internal.ca.crt
</span></span></code></pre></div><h2 id="分发证书-1">分发证书</h2>
<p><strong>Docker 证书设置</strong></p>
<p>所有节点执行</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo mkdir -p /etc/docker/certs.d/registry.code.internal
</span></span><span style="display:flex;"><span>sudo cp code.internal.ca.crt /etc/docker/certs.d/registry.code.internal/ca.crt
</span></span></code></pre></div><p><strong>CA 证书设置</strong>
需要执行 https pull 的节点执行:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>sudo cp code.internal.ca.crt /etc/ssl/certs/
</span></span></code></pre></div><h1 id="部署-gitlab-runner">部署 GitLab Runner</h1>
<p><code>GitLab Runner</code> 是实际的 CI/CD 任务的执行者。</p>
<p>首先根据 gitlab 的根证书创建 Secret:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>cp code.internal.ca.crt code.internal.crt
</span></span><span style="display:flex;"><span>cp code.internal.ca.crt registry.code.internal.crt
</span></span><span style="display:flex;"><span>cp mirrors.internal.ca.crt mirrors.internal.crt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>kubectl -n kube-gitlab create secret generic gitlab-runner.crt --from-file code.internal.crt --from-file registry.code.internal.crt --from-file mirrors.internal.crt
</span></span></code></pre></div><p>部署一个 Shared Runner:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>helm upgrade --install gitlab-runner-shared \
</span></span><span style="display:flex;"><span>   --namespace kube-gitlab \
</span></span><span style="display:flex;"><span>   --set gitlabUrl=&#34;https://code.internal&#34; \
</span></span><span style="display:flex;"><span>   --set runnerRegistrationToken=&#34;WIRNHWqSIlCi1vh0W3IGiDPMtgs58YGdB01aVLSCuHrPkCmY8xrOtUaRu97LEOiq&#34; \
</span></span><span style="display:flex;"><span>   --set certsSecretName=&#34;gitlab-runner.crt&#34; \
</span></span><span style="display:flex;"><span>   --set rbac.create=true \
</span></span><span style="display:flex;"><span>   --set rbac.clusterWideAccess=true \
</span></span><span style="display:flex;"><span>   --set runners.privileged=true \
</span></span><span style="display:flex;"><span>   --set runners.imagePullPolicy=&#34;always&#34; \
</span></span><span style="display:flex;"><span>   --set image=&#34;gitlab/gitlab-runner:alpine-v12.9.0&#34; \
</span></span><span style="display:flex;"><span>   gitlab/gitlab-runner
</span></span></code></pre></div><p>设置 <code>--set runners.imagePullPolicy=&quot;always&quot;</code> 表示每次 Runner 都去拉最新的 build 镜像.</p>
<h1 id="总结">总结</h1>
<p>至此，Kubernetes 高可用集群已搭建完成，基于 GitLab 的 DevOps 平台也已部署就位，接下来就是去使用、去深入。</p>
<p>当然，目前的集群已然存在很多问题，比如 <code>etcd</code> 是堆叠的，数据没有做进一步保障等，也需要进一步解决。</p>
<h1 id="参考">参考</h1>
<ul>
<li>Kubernetes HA, <a href="https://kubernetes.io/docs/setup/independent/high-availability">https://kubernetes.io/docs/setup/independent/high-availability</a></li>
<li>Kubernetes Ingress, <a href="https://kubernetes.io/docs/concepts/services-networking/ingress">https://kubernetes.io/docs/concepts/services-networking/ingress</a></li>
<li>Nginx Ingress, <a href="https://github.com/kubernetes/ingress-nginx">https://github.com/kubernetes/ingress-nginx</a></li>
<li>Helm, <a href="https://helm.sh/">https://helm.sh/</a></li>
<li>MetalLB, <a href="https://metallb.universe.tf/">https://metallb.universe.tf/</a></li>
<li>Rook, <a href="https://rook.io">https://rook.io</a></li>
<li>Harbor, <a href="https://goharbor.io/">https://goharbor.io/</a></li>
<li>GitLab Runner, <a href="https://gitlab.com/gitlab-org/gitlab-runner/">https://gitlab.com/gitlab-org/gitlab-runner/</a></li>
</ul>

      </div>

      <footer>
        
<div id="gitalk-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script>
  const gitalk = new Gitalk({
    clientID: '224c187d3c5d173aeddc',
    clientSecret: '09c21537075b08c9e5bfcf1f0f462451a5fa25fd',
    repo: 'maxsxu.github.io',
    owner: 'maxsxu',
    admin: ['maxsxu'],
    id: location.pathname, 
    distractionFreeMode: false 
  });
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('gitalk-container').innerHTML = 'Gitalk comments not available by default when the website is previewed locally.';
      return;
    }
    gitalk.render('gitalk-container');
  })();
</script>


        


        
      </footer>
    </article>

    <aside id="toc">
      <div class="toc-title">Table of Contents</div>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#系统规划">系统规划</a></li>
    <li><a href="#配置-ansible">配置 ansible</a></li>
    <li><a href="#同步时钟">同步时钟</a></li>
    <li><a href="#同步-hostname-和-hosts">同步 hostname 和 hosts</a></li>
    <li><a href="#同步基础软件和软件源">同步基础软件和软件源</a></li>
    <li><a href="#更新-kubeadm-kubectl-kubelet">更新 kubeadm, kubectl, kubelet</a></li>
    <li><a href="#同步-docker-配置">同步 docker 配置</a></li>
  </ul>

  <ul>
    <li><a href="#第一个控制节点初始化">第一个控制节点初始化</a></li>
    <li><a href="#部署cni">部署CNI</a></li>
    <li><a href="#其余控制节点加入">其余控制节点加入</a></li>
  </ul>

  <ul>
    <li><a href="#部署-rook-ceph-operator">部署 rook-ceph Operator</a></li>
    <li><a href="#创建-ceph-集群">创建 Ceph 集群</a></li>
    <li><a href="#创建-pool-和-storageclass">创建 Pool 和 StorageClass</a>
      <ul>
        <li><a href="#cephblockpool">CephBlockPool</a></li>
        <li><a href="#cephfilesystem">CephFileSystem</a></li>
      </ul>
    </li>
    <li><a href="#访问-ceph-dashboard">访问 Ceph Dashboard</a></li>
    <li><a href="#删除">删除</a></li>
  </ul>

  <ul>
    <li><a href="#导出根证书">导出根证书</a></li>
    <li><a href="#分发证书">分发证书</a></li>
  </ul>

  <ul>
    <li><a href="#导出根证书-1">导出根证书</a></li>
    <li><a href="#分发证书-1">分发证书</a></li>
  </ul>
</nav>
    </aside>

    
  </section>

      </div>

      <footer class="footer">
  <section class="container">
    
     © 2022
    
       · 
      Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/maxsxu/hugo-coder/">Coder</a>.
    
    
  </section>
</footer>

    </main>

  </body>

</html>
